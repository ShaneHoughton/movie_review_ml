{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame the Problem and Look at the Big Picture\n",
    "=====================================\n",
    "\n",
    "1. **Define the objective in business terms:** We have been hired by Magic Films for their new streaming service which provides access to a wide variety of movies. \n",
    "2. **How will your solution be used?** They want to eventually know what movies should be displayed and promoted on the homepage to attract users based on written reviews from viewers as well as what movies to remove from the website to save money on licensing fees.\n",
    "3. **What solutions are in place?** For certain businesses such as the restaurant business, they try and identify certain keywords in reviews to find out what they are doing well and their customers like and what they can improve on.\n",
    "4. **How should you frame this problem?** This is a supervised binary classification problem. Based on words and how they are used in a review we want to determine whether the review was `Positive` or `Negative`.\n",
    "5. **How should performance be measured? Is the performance measure aligned with the business objective?** Because we are not to identify as many positives as we can by risking false positives and we are not trying to identify all the negatives we can, we believe we should use F1 score to find a middle ground. We want to reduce false positives and false negatives as equally as possible.\n",
    "6. **What would be the minimum performance needed to reach the business objective?** We would like to get our score as close to 1.0 as possible. A goal is to get at least .85 or better.\n",
    "7. **What are comparable problems? Can you reuse experience or tools?** We have had multi-classification experience with the MNIST data set for trying to decide what a number a written digit or what article of clothing something is. More importantly, we previously created a binary-classification model that would try to predict whether there would be an early spring or not. We would not be able to directly reuse this model in particular because we are trying to identify words within a review to determine if the review was positive or not.\n",
    "8. **Is human expertise available?** We do not have anyone readily available but we are advised on the Readme to contact Andrew Maas, the organizer of the dataset, with any questions. We can also try and contact the user who uploaded the dataset to Kaggle.\n",
    "9. **How would you solve the problem manually?** The manual approach would to be to parse up the reviews and try and filter out words with negative or positive connotations in order to decide whether a review is positive or negative. This could be a problem with sarcastic reviews and reviews with spelling errors or slang.\n",
    "10. **List the assumptions you (or others) have made so far. Verify assumptions if possible.** We are assuming that the data set is all primarily english and has no spelling mistakes or errors in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for loading and splitting data\n",
    "def load_data(data):\n",
    "    \"\"\"Loads a dataset\"\"\"\n",
    "    return pd.read_csv(data)\n",
    "\n",
    "\n",
    "def split_labels(data, label_feature):\n",
    "    \"\"\"\n",
    "    Split the given column of the data, returning the full data set (without that\n",
    "    feature) and the split off feature.\n",
    "    \"\"\"\n",
    "    return data.drop(columns=label_feature), data[label_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Data\n",
    "==========\n",
    "\n",
    "1. **List the data you need and how much you need:** We have a dataset of IMDB movie reviews containing a combniation of positive and negative reviews for movies.\n",
    "2. **Find and document where you can get that data:** We got the data from: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download\n",
    "3. **Get access authorizations**: No authorization needed, the data is free to access on Kaggle.\n",
    "4. **Create a workspace**: This notebook.\n",
    "5. **Get the data**: Downloaded from Kaggle.\n",
    "6. **Convert the data to a format you can easily manipulate**: Already in CSV files so easy to use.\n",
    "7. **Ensure sensitive information is deleted or protected**: Data contains no sensitive information.\n",
    "8. **Check the size and type of data (time series, geographical, …)**: This categorical data set contains reviews written in text as well as a binary attribute indicating whether the review was positive or not. It contains 50,000 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('imdb_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 50000\n",
       "unique                                                49582\n",
       "top       Loved today's show!!! It was a variety and not...\n",
       "freq                                                      5\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`review` is a text object containing a written review of a certain film. There are 50000 of these reviews but only 49582 are unique. This will be the main feature that we will try and derive meaning and sentiment from as well as create new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        50000\n",
       "unique           2\n",
       "top       positive\n",
       "freq         25000\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sentiment` is our <b>target feature</b> and is a binary categorical variable. This is the label that indicates whether a review is a positive one or negative. A review is labeled simply as `positive` or `negative`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42, shuffle=True, stratify=data['sentiment'])\n",
    "train_set, valid_set = train_test_split(train_set, test_size=0.1, random_state=42, shuffle=True, stratify=train_set['sentiment'])\n",
    "X_train, y_train = split_labels(train_set, 'sentiment')\n",
    "X_test, y_test = split_labels(test_set, 'sentiment')\n",
    "X_valid, y_valid = split_labels(valid_set, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 18000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives = train_set[train_set['sentiment']=='positive'].reset_index()\n",
    "negatives = train_set[train_set['sentiment']=='negative'].reset_index()\n",
    "len(positives), len(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About half of the reviews in the training set are positive and the other half is negative, so both types are about equally represented. And it is also nice to know there are not any missing values in the dataset, which is important with just two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33890</td>\n",
       "      <td>This is an excellent stand-up DVD! Eddie Izzar...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35186</td>\n",
       "      <td>This movie has always been a favorite of mine ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41267</td>\n",
       "      <td>This is an excellent, fast paced thriller by W...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21302</td>\n",
       "      <td>I think this movie is my favorite movie. I am ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13974</td>\n",
       "      <td>Best Years of Our Lives is a film that slipped...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24506</td>\n",
       "      <td>It's 1913. A studio prop boy spies the actress...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25859</td>\n",
       "      <td>\"Showtime\" is a funny film starring funnymen R...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32515</td>\n",
       "      <td>This is a great TV miniseries of a classic nov...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26441</td>\n",
       "      <td>This is one of my favorite family movies. Love...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34065</td>\n",
       "      <td>If you think about it, it's nearly unbelievabl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             review sentiment\n",
       "0  33890  This is an excellent stand-up DVD! Eddie Izzar...  positive\n",
       "1  35186  This movie has always been a favorite of mine ...  positive\n",
       "2  41267  This is an excellent, fast paced thriller by W...  positive\n",
       "3  21302  I think this movie is my favorite movie. I am ...  positive\n",
       "4  13974  Best Years of Our Lives is a film that slipped...  positive\n",
       "5  24506  It's 1913. A studio prop boy spies the actress...  positive\n",
       "6  25859  \"Showtime\" is a funny film starring funnymen R...  positive\n",
       "7  32515  This is a great TV miniseries of a classic nov...  positive\n",
       "8  26441  This is one of my favorite family movies. Love...  positive\n",
       "9  34065  If you think about it, it's nearly unbelievabl...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a positive review, giving us an idea of what this kind of review will look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In Iran, the Islamic Revolution has shaped all parts of life, including everyday things. But people still go on living their lives, generally just doing the things you'd expect, like go to soccer matches to cheer on the national team as it's in the running to qualify for the World Cup. Except women aren't allowed to go to the soccer stadium to watch the game.<br /><br />A frequently funny little film follows the small group of women that were caught sneaking into the soccer stadium and the little group of bored soldiers assigned to guard them in a holding pen just outside the stadium. The absurdity of the situation, the simple wish of these women to cheer on the team (nothing subversive there), and little human touches about the lives of everyone adds up to quite a fine comment on humanity versus the ideology.<br /><br />Amateurish acting, good script and dialogue, a really enjoyable film. Bend It Like Beckham, sort of - a warm heart and a joy in the daily interests and pleasures of people.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives['review'][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a negative review, giving us an idea of what this kind of review will look like.\n",
    "It should be noted that there appear to be `HTML` elements peppered in some of the positive and negative reviews. In this example we see a `<br />` tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the html with a regular expression substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In Iran, the Islamic Revolution has shaped all parts of life, including everyday things. But people still go on living their lives, generally just doing the things you'd expect, like go to soccer matches to cheer on the national team as it's in the running to qualify for the World Cup. Except women aren't allowed to go to the soccer stadium to watch the game.  A frequently funny little film follows the small group of women that were caught sneaking into the soccer stadium and the little group of bored soldiers assigned to guard them in a holding pen just outside the stadium. The absurdity of the situation, the simple wish of these women to cheer on the team (nothing subversive there), and little human touches about the lives of everyone adds up to quite a fine comment on humanity versus the ideology.  Amateurish acting, good script and dialogue, a really enjoyable film. Bend It Like Beckham, sort of - a warm heart and a joy in the daily interests and pleasures of people.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re as re\n",
    "def remove_tags(string):\n",
    "    result = re.sub('<.*?>',' ', string)\n",
    "    return result\n",
    "\n",
    "positives['review'] = positives['review'].apply(lambda review : remove_tags(review))\n",
    "negatives['review'] = negatives['review'].apply(lambda review : remove_tags(review))\n",
    "\n",
    "positives['review'][42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is not great cinema. The film is cliche ridden and in many places it is a copy of the original Carrie. The parallels with the original film are striking but with an added improbability about the telekenetic powers being congenital.  I can't say that I disliked the film because it at least passed a couple of hours ... but these were passed as one would read an undemanding book on a long train journey. That book would not be great literature but would absorb one for a while and be forgotten soon afterwards in that it would blend in with the numerous other cliche ridden books. Likewise the film was okay whilst it lasted but is perhaps forgettable. At least I think it is. I cant remember much about it now!!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives['review'][13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin breaking down reviews and what words might entail a negative or positive review we might want to try and see what words are most popularly used..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the      239956\n",
       "and      123672\n",
       "a        116261\n",
       "of       108634\n",
       "to        93468\n",
       "          ...  \n",
       "made       4542\n",
       "being      4496\n",
       "make       4460\n",
       "it.        4449\n",
       "never      4425\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(' '.join(positives['review']).lower().split()).value_counts()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as expected, this approach will result in finding a lot of the words will be prepositions, determiners, conjuctions or very basic words found in speech. What might be more helpful is identifying key phrases, or less than the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "completely    937\n",
       "rest          934\n",
       "start         934\n",
       "friends       933\n",
       "episode       933\n",
       "             ... \n",
       "drama         743\n",
       "white         742\n",
       "directed      741\n",
       "course,       739\n",
       "including     739\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(' '.join(positives['review']).lower().split()).value_counts()[400:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "me.            994\n",
       "is,            994\n",
       "bad,           992\n",
       "performance    991\n",
       "line           986\n",
       "              ... \n",
       "bunch          794\n",
       "running        789\n",
       "cheap          789\n",
       "writing        785\n",
       "finally        784\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(' '.join(negatives['review']).lower().split()).value_counts()[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we get to words used frequently but less than the most common, we see ones that may be more telling like bad cheap writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try creating a new feature such as review length and see if there is any interesting patterns for negative and postitive review lengths. This may not be extremely helpful because we want to know the sentiment of a review based on its contents but with 2 features creating new ones could be useful. But we will try and create many new features by trying to implement `Bag of Words` eventuallly so it probably won't be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 704)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives['review_len'] = positives['review'].str.len()\n",
    "len(positives['review'][0]), positives['review_len'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1537, 1537)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives['review_len'] = negatives['review'].str.len()\n",
    "len(negatives['review'][0]), negatives['review_len'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18000.000000</td>\n",
       "      <td>18000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24994.566611</td>\n",
       "      <td>1302.842056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14364.484819</td>\n",
       "      <td>1009.492811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12564.750000</td>\n",
       "      <td>684.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24976.500000</td>\n",
       "      <td>954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37433.250000</td>\n",
       "      <td>1587.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49992.000000</td>\n",
       "      <td>13604.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index    review_len\n",
       "count  18000.000000  18000.000000\n",
       "mean   24994.566611   1302.842056\n",
       "std    14364.484819   1009.492811\n",
       "min        0.000000     65.000000\n",
       "25%    12564.750000    684.000000\n",
       "50%    24976.500000    954.000000\n",
       "75%    37433.250000   1587.000000\n",
       "max    49992.000000  13604.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18000.000000</td>\n",
       "      <td>18000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24800.754222</td>\n",
       "      <td>1279.153778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14397.278761</td>\n",
       "      <td>938.617424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12352.500000</td>\n",
       "      <td>697.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24722.500000</td>\n",
       "      <td>958.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37151.250000</td>\n",
       "      <td>1551.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49999.000000</td>\n",
       "      <td>8729.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index    review_len\n",
       "count  18000.000000  18000.000000\n",
       "mean   24800.754222   1279.153778\n",
       "std    14397.278761    938.617424\n",
       "min        3.000000     41.000000\n",
       "25%    12352.500000    697.000000\n",
       "50%    24722.500000    958.000000\n",
       "75%    37151.250000   1551.000000\n",
       "max    49999.000000   8729.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there does not seem like any apparent major differences between the two, but it looks like positive reviews are longer on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = pd.concat([positives, negatives], axis=0)['review_len']\n",
    "lengths.hist(bins=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be improved with a log transformation. Also its seems that someone wrote a particularly lengthy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(lengths).hist(bins=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be a helpful metric but what we truly want is a way to detect the sentiment in reviews based off processing the words in them instead of length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Data\n",
    "====\n",
    "1. **Data cleaning:** Fix/remove outliers (optional); We might possibly have to remove really long reviews? But that might not be necessary for evaluating sentiment.\n",
    "2. **Feature selection** (optional): Drop attributes that provide no useful information for the task\n",
    "3. **Feature engineering, where appropriate:** Discretize continuous features; Decompose features (categorical, date/time, ...), Add promising transformations of features (log(𝑥𝑥), √𝑥𝑥, 𝑥𝑥2, ...); Aggregate features into promising new features\n",
    "4. **Feature scaling:** standardize or normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words\n",
    "===========\n",
    "In order to solve our problem of having few features we are going to try and implement a Bag of Words approach. This will help our number of features grow exponentially, and we will do this as we prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, max_df=6000, lowercase=True, stop_words='english')\n",
    "vectorizer.fit(X_train['review'])\n",
    "X_train_vectorized = vectorizer.transform(X_train['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes a sparse matrix where each column is a specific word, each row is a review, and each entry has a 1 if the review contains that word. Similar concept to oneHot encoding but for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SGD: 0.8553055555555557\n",
      "Accuracy for Log_Reg: 0.8669444444444444\n"
     ]
    }
   ],
   "source": [
    "# Trying some classifiers on the vectorized data\n",
    "for clf_name, classifier in zip(['SGD', 'Log_Reg'],[SGDClassifier(), LogisticRegression(max_iter=200)]):\n",
    "    scores = cross_val_score(classifier, X_train_vectorized, y_train, cv=3, scoring='accuracy')\n",
    "    print('Accuracy for {}: {}'.format(clf_name, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary transformer?\n",
    "# save that vocabulary...\n",
    "# make a transformer that makes reviews into tokens \n",
    "# save that vocabulary to encode text\n",
    "# train the model on NN\n",
    "# https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = np.where(y_train == 'positive', 1, 0)\n",
    "y_test_int = np.where(y_test == 'positive', 1, 0)\n",
    "y_valid_int = np.where(y_valid == 'positive', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21794"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, max_df=6000, lowercase=True, stop_words='english')\n",
    "vectorizer.fit(X_train['review'])\n",
    "X_train_vectorized = vectorizer.transform(X_train['review'])\n",
    "X_valid_vectorized = vectorizer.transform(X_valid['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 21794)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_19/dense_35/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_19/dense_35/embedding_lookup_sparse/Reshape:0\", shape=(None, 1000), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_19/dense_35/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85/1125 [=>............................] - ETA: 4:10 - loss: 0.4536 - accuracy: 0.7956"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/h6ppm6td6hx2rlm1103pksph0000gn/T/ipykernel_62062/1312022953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(1000, activation='elu', input_shape=(None, 1, 21794)))\n",
    "model.add(keras.layers.Dense(500, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_normal'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_vectorized, y_train_int, epochs=10, verbose=1, validation_data=(X_valid_vectorized, y_valid_int), callbacks=[keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.9348 - accuracy: 0.8693: 0s - loss: 0.9420 - accuracy: 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9347573518753052, 0.8693000078201294]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test['review'])\n",
    "model.evaluate(X_test_vectorized, y_test_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with Keras\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# remove punctuation from review and lowercase it\n",
    "\tdoc = doc.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(doc, vocab):\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    " \n",
    "# load all docs in a dataframe\n",
    "def process_df_docs(df, vocab):\n",
    "\t# walk through all files in the folder\n",
    "\tfor entry in df['review']:\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(entry, vocab)\n",
    "  \n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(doc, vocab):\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_dir_docs(df, vocab):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor entry in df['review']:\n",
    "\t\tline = doc_to_line(entry, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_doc(positives['review'][42])\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154314\n",
      "[('the', 531274), ('and', 256205), ('of', 230364), ('to', 213250), ('is', 168136), ('in', 147400), ('it', 122418), ('this', 119423), ('that', 109035), ('br', 91147), ('was', 76246), ('as', 72998), ('for', 69259), ('with', 69191), ('movie', 66600), ('but', 65413), ('film', 59651), ('on', 53316), ('not', 48035), ('you', 47500), ('are', 46503), ('his', 45644), ('have', 44078), ('be', 42265), ('he', 41225), ('one', 40911), ('its', 39074), ('at', 37280), ('all', 36219), ('by', 35226), ('an', 34318), ('they', 33132), ('from', 32399), ('who', 32193), ('so', 31633), ('like', 31152), ('just', 27997), ('or', 27922), ('her', 27343), ('about', 27097), ('if', 26861), ('has', 26392), ('out', 26172), ('some', 24817), ('there', 24465), ('what', 24402), ('good', 22759), ('very', 22227), ('when', 22126), ('more', 22080)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_df_docs(train_set, vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71482\n"
     ]
    }
   ],
   "source": [
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens)) #71482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19961 20039\n"
     ]
    }
   ],
   "source": [
    "#19961 20039\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all training reviews\n",
    "positive_lines = process_dir_docs(positives, vocab)\n",
    "negative_lines = process_dir_docs(negatives, vocab)\n",
    "# summarize what we have\n",
    "print(len(positive_lines), len(negative_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5039, 4961)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_positives = test_set[test_set['sentiment']=='positive'].reset_index()\n",
    "test_negatives = test_set[test_set['sentiment']=='negative'].reset_index()\n",
    "len(test_positives), len(test_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/houghtons@moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb Cell 46'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=48'>49</a>\u001b[0m \u001b[39m# load all training reviews\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=49'>50</a>\u001b[0m positive_lines \u001b[39m=\u001b[39m process_docs(positives, vocab)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=50'>51</a>\u001b[0m negative_lines \u001b[39m=\u001b[39m process_docs(negatives, vocab)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=51'>52</a>\u001b[0m \u001b[39m# create the tokenizer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=52'>53</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mTokenizer()\n",
      "\u001b[1;32m/Users/houghtons@moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb Cell 46'\u001b[0m in \u001b[0;36mprocess_docs\u001b[0;34m(df, vocab)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=35'>36</a>\u001b[0m \u001b[39m# walk through all files in the folder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=37'>38</a>\u001b[0m \t\u001b[39m# load and clean the doc\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=38'>39</a>\u001b[0m \tline \u001b[39m=\u001b[39m doc_to_line(entry, vocab)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=39'>40</a>\u001b[0m \t\u001b[39m# add to list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=40'>41</a>\u001b[0m \tlines\u001b[39m.\u001b[39mappend(line)\n",
      "\u001b[1;32m/Users/houghtons@moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb Cell 46'\u001b[0m in \u001b[0;36mdoc_to_line\u001b[0;34m(doc, vocab)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdoc_to_line\u001b[39m(doc, vocab):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=26'>27</a>\u001b[0m \t\u001b[39m# clean doc\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=27'>28</a>\u001b[0m \ttokens \u001b[39m=\u001b[39m clean_doc(doc)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=28'>29</a>\u001b[0m \t\u001b[39m# filter by vocab\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=29'>30</a>\u001b[0m \ttokens \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m w \u001b[39min\u001b[39;00m vocab]\n",
      "\u001b[1;32m/Users/houghtons@moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb Cell 46'\u001b[0m in \u001b[0;36mclean_doc\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_doc\u001b[39m(doc):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=14'>15</a>\u001b[0m \t\u001b[39m# remove punctuation from review and lowercase it\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=15'>16</a>\u001b[0m \tdoc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mtranslate(\u001b[39mstr\u001b[39;49m\u001b[39m.\u001b[39;49mmaketrans(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, string\u001b[39m.\u001b[39;49mpunctuation))\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=16'>17</a>\u001b[0m \t\u001b[39m# split into tokens by white space\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/houghtons%40moravian.edu/Desktop/cs390/final/movie_review_ml/final_project.ipynb#ch0000046?line=17'>18</a>\u001b[0m \ttokens \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39msplit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# remove punctuation from review and lowercase it\n",
    "\tdoc = doc.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(doc, vocab):\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# load all docs in a dataframe\n",
    "def process_docs(df, vocab):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor entry in df['review']:\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(entry, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all training reviews\n",
    "positive_lines = process_docs(positives, vocab)\n",
    "negative_lines = process_docs(negatives, vocab)\n",
    "# create the tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "docs = negative_lines + positive_lines\n",
    "tokenizer.fit_on_texts(docs)\n",
    "# encode training data set\n",
    "Xtrain = tokenizer.texts_to_matrix(docs, mode='freq')\n",
    "ytrain = np.array([0 for _ in range(20000)] + [1 for _ in range(20000)])\n",
    "\n",
    "# load all test reviews\n",
    "positive_lines = process_docs(test_positives, vocab)\n",
    "negative_lines = process_docs(test_negatives, vocab)\n",
    "docs = negative_lines + positive_lines\n",
    "# encode training data set\n",
    "Xtest = tokenizer.texts_to_matrix(docs, mode='freq')\n",
    "ytest = np.array([0 for _ in range(5000)] + [1 for _ in range(5000)])\n",
    "\n",
    "n_words = Xtest.shape[1]\n",
    "# define network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 48.460001\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=5, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = keras.preprocessing.text.Tokenizer()\n",
    "tok.fit_on_texts(train_set['review'])\n",
    "mat = tok.texts_to_matrix(train_set['review'])\n",
    "n_words = mat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 106878)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 11:20:21.426402: W tensorflow/core/framework/op_kernel.cc:1722] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node binary_crossentropy/Cast\n (defined at /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/losses.py:1797)\n]] [Op:__inference_train_function_4725]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node binary_crossentropy/Cast:\nIn[0] IteratorGetNext (defined at /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py:866)\n\nOperation defined at: (most recent call last)\n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 345, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2898, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2944, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3169, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3361, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/var/folders/73/h6ppm6td6hx2rlm1103pksph0000gn/T/ipykernel_55778/3430352926.py\", line 6, in <module>\n>>>     model.fit(mat,y_train, epochs=5, verbose=2)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/losses.py\", line 1797, in binary_crossentropy\n>>>     y_true = tf.cast(y_true, y_pred.dtype)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/h6ppm6td6hx2rlm1103pksph0000gn/T/ipykernel_55778/3430352926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# loss, acc = model.evaluate(Xtest, y_test, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# print('Test Acc: %f'%(acc*100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node binary_crossentropy/Cast\n (defined at /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/losses.py:1797)\n]] [Op:__inference_train_function_4725]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node binary_crossentropy/Cast:\nIn[0] IteratorGetNext (defined at /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py:866)\n\nOperation defined at: (most recent call last)\n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 345, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2898, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2944, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3169, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3361, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/var/folders/73/h6ppm6td6hx2rlm1103pksph0000gn/T/ipykernel_55778/3430352926.py\", line 6, in <module>\n>>>     model.fit(mat,y_train, epochs=5, verbose=2)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/losses.py\", line 245, in call\n>>>     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/losses.py\", line 1797, in binary_crossentropy\n>>>     y_true = tf.cast(y_true, y_pred.dtype)\n>>> "
     ]
    }
   ],
   "source": [
    "# define network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(mat,y_train, epochs=5, verbose=2)\n",
    "# loss, acc = model.evaluate(Xtest, y_test, verbose=0)\n",
    "# print('Test Acc: %f'%(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Sklearn\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 88840)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_set['review'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3101"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36000, 88840)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveHTMLTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer class that removes HTML tags from strings\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X.apply(lambda review : remove_tags(review))\n",
    "        return X\n",
    "\n",
    "    def remove_tags(string):\n",
    "        result = re.sub('<.*?>',' ', string)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24839    The movie starts off as we see a footage of a ...\n",
       "33890    This is an excellent stand-up DVD! Eddie Izzar...\n",
       "4692     Though I've yet to review the movie in about t...\n",
       "34202    As a big fan of David Mamet's films and plays,...\n",
       "29985    This move is bad on so many levels I don't eve...\n",
       "                               ...                        \n",
       "17522    Despite being quite far removed from my expect...\n",
       "23897    Holy crap this movie was bad. I watched it jus...\n",
       "43410    This movie is horrible. Everything in it has b...\n",
       "24457    Sorry, I don't have much time to write. I am n...\n",
       "15389    I really wanted to like this movie, but ended ...\n",
       "Name: review, Length: 36000, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_trans = RemoveHTMLTransformer()\n",
    "html_trans.transform(train_set['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8478"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('HTML_remover', RemoveHTMLTransformer('review')),\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "text_clf.fit(train_set['review'], train_set['sentiment'])\n",
    "\n",
    "predicted = text_clf.predict(test_set['review'])\n",
    "np.mean(predicted == test_set['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__alpha', 'clf__average', 'clf__class_weight', 'clf__early_stopping', 'clf__epsilon', 'clf__eta0', 'clf__fit_intercept', 'clf__l1_ratio', 'clf__learning_rate', 'clf__loss', 'clf__max_iter', 'clf__n_iter_no_change', 'clf__n_jobs', 'clf__penalty', 'clf__power_t', 'clf__random_state', 'clf__shuffle', 'clf__tol', 'clf__validation_fraction', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/h6ppm6td6hx2rlm1103pksph0000gn/T/ipykernel_52661/2421250105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msgd_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msgd_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_param_grid = [{\n",
    "    'tfidf__norm':['l1', 'l2'],\n",
    "    'tfidf__sublinear_tf':[True, False],\n",
    "    'clf__alpha': np.linspace(1e-6, 1e-4, 5),\n",
    "    'vect__stop_words':[None, 'english'],\n",
    "    'vect__max_df':[0.5, 0.75, 1.0],\n",
    "}]\n",
    "sgd_search = GridSearchCV(\n",
    "    text_clf, text_param_grid,\n",
    "    cv=5, scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "sgd_search.fit(train_set['review'], train_set['sentiment'], verbose=2)\n",
    "sgd_search.best_params_, sgd_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_param_grid = [{\n",
    "    'tfidf__norm':['l1', 'l2'],\n",
    "    'tfidf__sublinear_tf':[True, False],\n",
    "    'clf__alpha': np.linspace(1e-6, 1e-4, 5),\n",
    "    'vect__stop_words':[None, 'english'],\n",
    "    'vect__max_df':[0.5, 0.75, 1.0],\n",
    "}]\n",
    "sgd_search = GridSearchCV(\n",
    "    text_clf, text_param_grid,\n",
    "    cv=5, scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "sgd_search.fit(train_set['review'], train_set['sentiment'], verbose=2)\n",
    "sgd_search.best_params_, sgd_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
